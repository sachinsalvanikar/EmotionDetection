{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/sachinsalvanikar/Desktop/USF/DSP/Project/Emotion Detection Project/CNN/pra.jpg'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.image import img_to_array\n",
    "import imutils\n",
    "import cv2\n",
    "import os\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "\n",
    "# parameters for loading data and images\n",
    "detection_model_path = os.getcwd() + '/haarcascade/haarcascade_frontalface_default.xml'\n",
    "emotion_model_path = os.getcwd() + '/model/_mini_XCEPTION.56-0.64.hdf5'\n",
    "img_path = os.getcwd() + '/pra.jpg'\n",
    "img_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-parameters for bounding boxes shape\n",
    "face_detection = cv2.CascadeClassifier(detection_model_path)\n",
    "emotion_classifier = load_model(emotion_model_path, compile=False)\n",
    "EMOTIONS = [\"angry\",\"disgust\",\"scared\", \"happy\", \"sad\", \"surprised\",\"neutral\"]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imutils import face_utils\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import dlib\n",
    "import cv2\n",
    "\n",
    "# initialize dlib's face detector (HOG-based) and then create\n",
    "# the facial landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rectangles[[(170, 420) (491, 741)]]\n"
     ]
    }
   ],
   "source": [
    "#reading the frame from the image\n",
    "\n",
    "orig_frame = cv2.imread(img_path)\n",
    "frame = cv2.imread(img_path,0)\n",
    "\n",
    "\n",
    "cv2.imshow('Original_face', orig_frame)\n",
    "\n",
    "# detect faces in the grayscale image\n",
    "rects = detector(frame, 1)\n",
    "\n",
    "# detecting the face from teh image        \n",
    "\n",
    "faces = face_detection.detectMultiScale(frame,scaleFactor=1.1,minNeighbors=5,minSize=(30,30),flags=cv2.CASCADE_SCALE_IMAGE)\n",
    " \n",
    "print(rects)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the face reactangle coordinates \n",
    "for (i, rect) in enumerate(rects):\n",
    "         #Cut and save face\n",
    "    (X, Y, W, H) = face_utils.rect_to_bb(rect)\n",
    "        #print(Y)\n",
    "\n",
    "frameClone = frame.copy()\n",
    "\n",
    "# Get the frame of the face \n",
    "roi = frame[Y:Y + H, X:X + W]\n",
    "roi = cv2.resize(roi, (48, 48))\n",
    "roi = roi.astype(\"float\") / 255.0\n",
    "roi = img_to_array(roi)\n",
    "roi = np.expand_dims(roi, axis=0)\n",
    "\n",
    "#Check the prediction from the saved model:\n",
    "preds = emotion_classifier.predict(roi)[0]\n",
    "emotion_probability = np.max(preds)\n",
    "label = EMOTIONS[preds.argmax()]      \n",
    "         \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the output frame \n",
    "for (i, (emotion, prob)) in enumerate(zip(EMOTIONS, preds)):\n",
    "        # construct the label text\n",
    "    text = \"{}: {:.2f}%\".format(emotion, prob * 100)\n",
    "    w = int(prob * 300)\n",
    "    cv2.putText(frameClone, label, (X, Y - 10),\n",
    "    cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "    cv2.rectangle(frameClone, (X, Y), (X + W, Y + H),\n",
    "                          (0, 0, 255), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Writing the output image in to a file\n",
    "cv2.imwrite('test_output/'+img_path.split('/')[-1],frameClone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('Predict_face', frameClone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

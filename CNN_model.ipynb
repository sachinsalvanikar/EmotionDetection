{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/CNN/fer2013_New.csv'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os \n",
    " \n",
    "#Load Dataset Path \n",
    "dataset_path = os.getcwd() + \"/fer2013_New.csv\"\n",
    "image_size=(48,48)\n",
    "dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(dataset_path)\n",
    "pixels = data['pixels'].tolist()\n",
    "width, height = 48, 48\n",
    "faces = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(35340, 6)\n",
      "(35340, 48, 48, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:12: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# load Dataset \n",
    "i=1\n",
    "for pixel_seq in pixels:\n",
    "    face = [int(pixel) for pixel in pixel_seq.split(' ')]\n",
    "    face = np.asarray(face).reshape(width, height)\n",
    "    face = cv2.resize(face.astype('uint8'),image_size)\n",
    "    faces.append(face.astype('float32'))    \n",
    "    i=i+1\n",
    "    #break\n",
    "faces = np.asarray(faces)\n",
    "faces = np.expand_dims(faces, -1)\n",
    "emotions = pd.get_dummies(data['emotion']).as_matrix()    \n",
    "\n",
    "#Check the Dataset shape\n",
    "print(emotions.shape)\n",
    "print(faces.shape)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It is a standard way to pre-process images by scaling them between -1 to 1. \n",
    "# Images is scaled to [0,1] by dividing it by 255\n",
    "\n",
    "def preprocess_input(x, v2=True):\n",
    "    x = x.astype('float32')\n",
    "    x = x / 255.0\n",
    "    if v2:\n",
    "        x = x - 0.5\n",
    "        x = x * 2.0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset in to Train and Test in 80:20 ratio\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#faces, emotions = load_data()\n",
    "faces = preprocess_input(faces)\n",
    "xtrain, xtest,ytrain,ytest = train_test_split(faces, emotions,test_size=0.2,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the library\n",
    "from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Activation, Convolution2D, Dropout, Conv2D\n",
    "from keras.layers import AveragePooling2D, BatchNormalization\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Flatten\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import SeparableConv2D\n",
    "from keras import layers\n",
    "from keras.regularizers import l2\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "batch_size = 32\n",
    "num_epochs = 110\n",
    "input_shape = (48, 48, 1)\n",
    "verbose = 1\n",
    "num_classes = 6\n",
    "patience = 50\n",
    "base_path = 'models/'\n",
    "l2_regularization=0.01\n",
    " \n",
    "# data generator\n",
    "data_generator = ImageDataGenerator(\n",
    "                        featurewise_center=False,\n",
    "                        featurewise_std_normalization=False,\n",
    "                        rotation_range=10,\n",
    "                        width_shift_range=0.1,\n",
    "                        height_shift_range=0.1,\n",
    "                        zoom_range=.1,\n",
    "                        horizontal_flip=True)\n",
    " \n",
    "# model parameters\n",
    "regularization = l2(l2_regularization)\n",
    " \n",
    "# base\n",
    "img_input = Input(input_shape)\n",
    "x = Conv2D(8, (3, 3), strides=(1, 1), kernel_regularizer=regularization, use_bias=False)(img_input)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = Conv2D(8, (3, 3), strides=(1, 1), kernel_regularizer=regularization, use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 48, 48, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 46, 46, 8)    72          input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 46, 46, 8)    32          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_10 (Activation)      (None, 46, 46, 8)    0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 44, 44, 8)    576         activation_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 44, 44, 8)    32          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_11 (Activation)      (None, 44, 44, 8)    0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_7 (SeparableCo (None, 44, 44, 128)  1096        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 44, 44, 128)  512         separable_conv2d_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_12 (Activation)      (None, 44, 44, 128)  0           batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_8 (SeparableCo (None, 44, 44, 128)  17536       activation_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 44, 44, 128)  512         separable_conv2d_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 22, 22, 128)  1024        activation_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 22, 22, 128)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 22, 22, 128)  512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 22, 22, 128)  0           max_pooling2d_4[0][0]            \n",
      "                                                                 batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 22, 22, 6)    6918        add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 6)            0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Activation)        (None, 6)            0           global_average_pooling2d_4[0][0] \n",
      "==================================================================================================\n",
      "Total params: 28,822\n",
      "Trainable params: 28,022\n",
      "Non-trainable params: 800\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Build the model\n",
    "residual = Conv2D(128, (1, 1), strides=(2, 2),padding='same', use_bias=False)(x)\n",
    "residual = BatchNormalization()(residual)\n",
    "x = SeparableConv2D(128, (3, 3), padding='same',kernel_regularizer=regularization,use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Activation('relu')(x)\n",
    "x = SeparableConv2D(128, (3, 3), padding='same',kernel_regularizer=regularization,use_bias=False)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
    "x = layers.add([x, residual])\n",
    "x = Conv2D(num_classes, (3, 3), padding='same')(x)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "output = Activation('softmax',name='predictions')(x)\n",
    " \n",
    "model = Model(img_input, output)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/110\n",
      "884/883 [==============================] - 923s 1s/step - loss: 1.7835 - acc: 0.2741 - val_loss: 1.8648 - val_acc: 0.2088\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.86476, saving model to /home/jupyter/CNN/model/model.01-0.21.hdf5\n",
      "Epoch 2/110\n",
      "884/883 [==============================] - 914s 1s/step - loss: 1.6842 - acc: 0.3113 - val_loss: 1.7112 - val_acc: 0.2842\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.86476 to 1.71118, saving model to /home/jupyter/CNN/model/model.02-0.28.hdf5\n",
      "Epoch 3/110\n",
      "884/883 [==============================] - 915s 1s/step - loss: 1.6516 - acc: 0.3288 - val_loss: 2.0070 - val_acc: 0.2821\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.71118\n",
      "Epoch 4/110\n",
      "884/883 [==============================] - 909s 1s/step - loss: 1.6324 - acc: 0.3414 - val_loss: 1.6678 - val_acc: 0.3291\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.71118 to 1.66782, saving model to /home/jupyter/CNN/model/model.04-0.33.hdf5\n",
      "Epoch 5/110\n",
      "884/883 [==============================] - 911s 1s/step - loss: 1.5746 - acc: 0.3754 - val_loss: 2.4170 - val_acc: 0.3026\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 1.63510\n",
      "Epoch 8/110\n",
      "884/883 [==============================] - 915s 1s/step - loss: 1.5583 - acc: 0.3830 - val_loss: 1.6430 - val_acc: 0.3718\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.63510\n",
      "Epoch 9/110\n",
      "884/883 [==============================] - 910s 1s/step - loss: 1.5476 - acc: 0.3882 - val_loss: 1.7825 - val_acc: 0.3176\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 1.63510\n",
      "Epoch 10/110\n",
      "884/883 [==============================] - 912s 1s/step - loss: 1.5363 - acc: 0.3938 - val_loss: 1.6444 - val_acc: 0.3664\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.63510\n",
      "Epoch 11/110\n",
      "884/883 [==============================] - 834s 943ms/step - loss: 1.5220 - acc: 0.4013 - val_loss: 2.2125 - val_acc: 0.2828\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 1.63510\n",
      "Epoch 12/110\n",
      "884/883 [==============================] - 806s 912ms/step - loss: 1.5092 - acc: 0.4081 - val_loss: 1.5135 - val_acc: 0.4116\n",
      "\n",
      "Epoch 00012: val_loss improved from 1.63510 to 1.51350, saving model to /home/jupyter/CNN/model/model.12-0.41.hdf5\n",
      "Epoch 13/110\n",
      "884/883 [==============================] - 800s 905ms/step - loss: 1.5011 - acc: 0.4159 - val_loss: 1.9407 - val_acc: 0.3305\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 1.51350\n",
      "Epoch 14/110\n",
      "884/883 [==============================] - 796s 900ms/step - loss: 1.4929 - acc: 0.4224 - val_loss: 1.7096 - val_acc: 0.3432\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 1.51350\n",
      "Epoch 15/110\n",
      "884/883 [==============================] - 783s 886ms/step - loss: 1.4885 - acc: 0.4210 - val_loss: 1.8987 - val_acc: 0.2949\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 1.51350\n",
      "Epoch 16/110\n",
      "884/883 [==============================] - 785s 888ms/step - loss: 1.4736 - acc: 0.4281 - val_loss: 1.5823 - val_acc: 0.3827\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 1.51350\n",
      "Epoch 17/110\n",
      "884/883 [==============================] - 777s 879ms/step - loss: 1.4689 - acc: 0.4271 - val_loss: 1.8319 - val_acc: 0.3256\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 1.51350\n",
      "Epoch 18/110\n",
      "884/883 [==============================] - 780s 883ms/step - loss: 1.4610 - acc: 0.4373 - val_loss: 1.5420 - val_acc: 0.3898\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 1.51350\n",
      "Epoch 19/110\n",
      "884/883 [==============================] - 773s 875ms/step - loss: 1.4537 - acc: 0.4422 - val_loss: 1.5866 - val_acc: 0.3853\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 1.51350\n",
      "Epoch 20/110\n",
      "884/883 [==============================] - 776s 878ms/step - loss: 1.4492 - acc: 0.4406 - val_loss: 1.7976 - val_acc: 0.3621\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 1.51350\n",
      "Epoch 21/110\n",
      "884/883 [==============================] - 785s 888ms/step - loss: 1.4434 - acc: 0.4419 - val_loss: 1.5754 - val_acc: 0.4046\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.51350\n",
      "Epoch 22/110\n",
      "884/883 [==============================] - 776s 877ms/step - loss: 1.4363 - acc: 0.4452 - val_loss: 1.5229 - val_acc: 0.4148\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.51350\n",
      "Epoch 23/110\n",
      "884/883 [==============================] - 777s 878ms/step - loss: 1.4295 - acc: 0.4521 - val_loss: 1.4154 - val_acc: 0.4469\n",
      "\n",
      "Epoch 00023: val_loss improved from 1.51350 to 1.41538, saving model to /home/jupyter/CNN/model/model.23-0.45.hdf5\n",
      "Epoch 24/110\n",
      "884/883 [==============================] - 777s 879ms/step - loss: 1.4219 - acc: 0.4523 - val_loss: 1.6646 - val_acc: 0.3646\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.41538\n",
      "Epoch 25/110\n",
      "884/883 [==============================] - 779s 881ms/step - loss: 1.4223 - acc: 0.4539 - val_loss: 2.3198 - val_acc: 0.3408\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.41538\n",
      "Epoch 26/110\n",
      "884/883 [==============================] - 777s 879ms/step - loss: 1.4162 - acc: 0.4542 - val_loss: 1.3867 - val_acc: 0.4721\n",
      "\n",
      "Epoch 00026: val_loss improved from 1.41538 to 1.38670, saving model to /home/jupyter/CNN/model/model.26-0.47.hdf5\n",
      "Epoch 27/110\n",
      "884/883 [==============================] - 779s 882ms/step - loss: 1.4090 - acc: 0.4579 - val_loss: 1.5316 - val_acc: 0.4147\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.38670\n",
      "Epoch 28/110\n",
      "884/883 [==============================] - 788s 892ms/step - loss: 1.4061 - acc: 0.4563 - val_loss: 1.4944 - val_acc: 0.4311\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.38670\n",
      "Epoch 29/110\n",
      "884/883 [==============================] - 779s 881ms/step - loss: 1.3978 - acc: 0.4650 - val_loss: 1.6183 - val_acc: 0.3738\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.38670\n",
      "Epoch 30/110\n",
      "884/883 [==============================] - 783s 886ms/step - loss: 1.3978 - acc: 0.4644 - val_loss: 1.6607 - val_acc: 0.3885\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 1.38670\n",
      "Epoch 31/110\n",
      "884/883 [==============================] - 766s 867ms/step - loss: 1.3951 - acc: 0.4672 - val_loss: 1.6337 - val_acc: 0.3870\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 1.38670\n",
      "Epoch 32/110\n",
      "884/883 [==============================] - 776s 878ms/step - loss: 1.3887 - acc: 0.4685 - val_loss: 1.4630 - val_acc: 0.4477\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.38670\n",
      "Epoch 33/110\n",
      "884/883 [==============================] - 779s 882ms/step - loss: 1.3853 - acc: 0.4691 - val_loss: 1.3917 - val_acc: 0.4663\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 1.38670\n",
      "Epoch 34/110\n",
      "884/883 [==============================] - 775s 876ms/step - loss: 1.3834 - acc: 0.4696 - val_loss: 1.4325 - val_acc: 0.4437\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.38670\n",
      "Epoch 35/110\n",
      "752/883 [========================>.....] - ETA: 1:45 - loss: 1.3774 - acc: 0.4739"
     ]
    }
   ],
   "source": [
    "# callbacks\n",
    "base_path =os.getcwd() + \"/model/\"\n",
    "log_file_path = base_path + '_emotion_training.log'\n",
    "csv_logger = CSVLogger(log_file_path, append=False)\n",
    "early_stop = EarlyStopping('val_loss', patience=patience)\n",
    "reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1, patience=int(patience/4), verbose=1)\n",
    "trained_models_path = base_path + 'model'\n",
    "model_names = trained_models_path + '.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
    "model_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1,save_best_only=True)\n",
    "callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]\n",
    "\n",
    "model.fit_generator(data_generator.flow(xtrain, ytrain,batch_size),\n",
    "                        steps_per_epoch=len(xtrain) / batch_size,\n",
    "                        epochs=num_epochs, verbose=1, callbacks=callbacks,\n",
    "                        validation_data=(xtest,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating the model.\n",
    "model.evaluate(xtest, ytest,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model with .h5 extension.\n",
    "# once We will save this trained model then we can reuse it on other system.\n",
    "model.save_weights('CNNModel.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

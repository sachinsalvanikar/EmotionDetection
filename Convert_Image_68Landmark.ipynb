{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt \n",
    "from datetime import datetime\n",
    "from scipy import misc\n",
    "from scipy import ndimage, misc\n",
    "from sklearn.datasets import fetch_mldata\n",
    "from PIL import Image\n",
    "import matplotlib\n",
    "import cv2\n",
    "import random\n",
    "import math\n",
    "import dlib\n",
    "import argparse\n",
    "import imutils\n",
    "import itertools\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(os.getcwd() + \"/Dlib/shape_predictor_68_face_landmarks.dat\")\n",
    "data = {} #Make dictionary for all values\n",
    "clf = SVC(kernel='linear', probability=True, tol=1e-3)#, verbose = True) #Set the classifier as a support vector machines with polynomial kernel       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "def get_files():\n",
    "    training= os.getcwd() + \"/Dataset/Images/Training/\"\n",
    "    PrivateTest = os.getcwd() + \"/Dataset/Images/PrivateTest/\"\n",
    "    PublicTest = os.getcwd() + \"/Dataset/Images/PublicTest/\"\n",
    "    \n",
    "    return training, PrivateTest,PublicTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_landmarks(image):\n",
    "    detections = detector(image, 1)\n",
    "    for k,d in enumerate(detections): #For all detected face instances individually\n",
    "        shape = predictor(image, d) #Draw Facial Landmarks with the predictor class\n",
    "        xlist = []\n",
    "        ylist = []\n",
    "        for i in range(1,68): #Store X and Y coordinates in two lists\n",
    "            xlist.append(float(shape.part(i).x))\n",
    "            ylist.append(float(shape.part(i).y))\n",
    "        xmean = np.mean(xlist)\n",
    "        ymean = np.mean(ylist)\n",
    "        xcentral = [(x-xmean) for x in xlist]\n",
    "        ycentral = [(y-ymean) for y in ylist]\n",
    "        landmarks_vectorised = []\n",
    "        for x, y, w, z in zip(xcentral, ycentral, xlist, ylist):\n",
    "            landmarks_vectorised.append(w)\n",
    "            landmarks_vectorised.append(z)\n",
    "            meannp = np.asarray((ymean,xmean))\n",
    "            coornp = np.asarray((z,w))\n",
    "            dist = np.linalg.norm(coornp-meannp)\n",
    "            landmarks_vectorised.append(dist)\n",
    "            landmarks_vectorised.append((math.atan2(y, x)*360)/(2*math.pi))\n",
    "        data['landmarks_vectorised'] = landmarks_vectorised\n",
    "    if len(detections) < 1:\n",
    "        data['landmarks_vestorised'] = \"error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = []\n",
    "training_labels = []\n",
    "PrivateTest_data = []\n",
    "PrivateTest_labels = []\n",
    "PublicTest_labels = []\n",
    "PublicTest_data = []\n",
    "\n",
    "#Or set this to whatever you named the downloaded file\n",
    "count = 0\n",
    "clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "#csv_pred = pd.read_csv('/Users/sachinsalvanikar/Desktop/USF/DSP/Project/FaceDetection/Check/Dataset-HappySad_test.csv')\n",
    "#csv_train = pd.read_csv('/Users/sachinsalvanikar/Desktop/USF/DSP/Project/FaceDetection/Check/Dataset_HappySad_train.csv')\n",
    "csv_PrivateTest = pd.read_csv(os.getcwd() + \"/Dataset/Images/PrivateTest.csv\")\n",
    "csv_train = pd.read_csv(os.getcwd() + \"/Dataset/Images/Training.csv\")\n",
    "csv_PublicTest = pd.read_csv(os.getcwd() + \"/Dataset/Images/PublicTest.csv\")\n",
    "\n",
    "training, PrivateTest, PublicTest  = get_files()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Traning dataset...\n"
     ]
    }
   ],
   "source": [
    "#Append data to training list, and generate labels 0-7\n",
    "print(\"Creating Traning dataset...\")\n",
    "for i in range(len(csv_train)):\n",
    "    x=csv_train['Image'][i]\n",
    "    y='.jpg'\n",
    "    filename=str(training)+str(x)+y\n",
    "    #filename=str(training)+str(x)\n",
    "    #print(filename)\n",
    "    image = cv2.imread(filename)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #convert to grayscale\n",
    "    clahe_image = clahe.apply(gray)\n",
    "    get_landmarks(clahe_image)\n",
    "    data['landmarks_vectorised']\n",
    "    if data['landmarks_vectorised'] == \"error\":\n",
    "        print(\"no face detected on this one\")\n",
    "    else:\n",
    "        training_data.append(data['landmarks_vectorised'])#append image array to training data list\n",
    "        z =csv_train['Label'][i]\n",
    "        training_labels.append(z.astype(int))\n",
    "                        \n",
    "\n",
    "np.save(os.getcwd() + \"/Dataset/Image_Landmarks/Training/Training_labels.npy\", training_labels)\n",
    "np.save(os.getcwd() + \"/Dataset/Image_Landmarks/Training/Training_landmarks.npy\", training_data)                       \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PrivateTest dataset...\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating PrivateTest dataset...\")\n",
    "for i in range(len(csv_PrivateTest)):\n",
    "    x=csv_PrivateTest['Image'][i]\n",
    "    y='.jpg'\n",
    "    filename=str(PrivateTest)+str(x)+y\n",
    "    #filename=str(prediction)+str(x)\n",
    "    #print(filename)\n",
    "    image = cv2.imread(filename)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #convert to grayscale\n",
    "    clahe_image = clahe.apply(gray)\n",
    "    get_landmarks(clahe_image)\n",
    "    data['landmarks_vectorised']\n",
    "    if data['landmarks_vectorised'] == \"error\":\n",
    "        print(\"no face detected on this one\")\n",
    "    else:\n",
    "        PrivateTest_data.append(data['landmarks_vectorised'])#append image array to training data list\n",
    "        z =csv_PrivateTest['Label'][i]\n",
    "        PrivateTest_labels.append(z.astype(int))\n",
    "        \n",
    "\n",
    "np.save(os.getcwd() + \"/Dataset/Image_Landmarks/PrivateTest/PrivateTest_labels.npy\", PrivateTest_labels)\n",
    "np.save(os.getcwd() + \"/Dataset/Image_Landmarks/PrivateTest/PrivateTest_landmarks.npy\", PrivateTest_data)                       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating PrivateTest dataset...\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating PrivateTest dataset...\")\n",
    "for i in range(len(csv_PublicTest)):\n",
    "    x=csv_PublicTest['Image'][i]\n",
    "    y='.jpg'\n",
    "    filename=str(PublicTest)+str(x)+y\n",
    "    #filename=str(prediction)+str(x)\n",
    "    #print(filename)\n",
    "    image = cv2.imread(filename)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) #convert to grayscale\n",
    "    clahe_image = clahe.apply(gray)\n",
    "    get_landmarks(clahe_image)\n",
    "    data['landmarks_vectorised']\n",
    "    if data['landmarks_vectorised'] == \"error\":\n",
    "        print(\"no face detected on this one\")\n",
    "    else:\n",
    "        PublicTest_data.append(data['landmarks_vectorised'])#append image array to training data list\n",
    "        z =csv_PublicTest['Label'][i]\n",
    "        PublicTest_labels.append(z.astype(int))\n",
    "        \n",
    "\n",
    "np.save(os.getcwd() + \"/Dataset/Image_Landmarks/PublicTest/PublicTest_labels.npy\", PublicTest_labels)\n",
    "np.save(os.getcwd() + \"/Dataset/Image_Landmarks/PublicTest/PublicTest_landmarks.npy\", PublicTest_data)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
